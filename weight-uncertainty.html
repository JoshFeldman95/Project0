<!DOCTYPE html>
<html>
    <head>
        <title>Weight Uncertainty</title>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
        <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.5/css/bootstrap.min.css" rel="stylesheet"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="stylesheet" href="main.css">
    </head>
    <body>
        <div class='container'>
            <div class='row'>
                <div class = 'col-4'>
                    <div class="col-12" id="sticky-sidebar">
                        <a class="sidebar-title" href="index.html">Josh Feldman Blog</a>
                        <nav class="nav nav-pills flex-column">
                            <a class="nav-link" href="index.html">Home</a>
                            <a class="nav-link" href="about.html">About Me</a>
                            <a class="nav-link" href="index.html">Posts</a>
                            <ul class="sidebar-list">
                                <li>
                                    <a class="nav-link" href="https://sites.google.com/view/cs109lendingclubgroup52/home?authuser=0">"Fair" Investment Strategies</a>
                                </li>
                                <li>
                                    <a class="nav-link" href="http://blog.petrieflom.law.harvard.edu/2018/11/30/the-tricky-task-of-defining-ai-in-the-law/">Defining AI</a>
                                </li>
                                <li>
                                    <a class="nav-link active" href="weight-uncertainty.html">Bayesian Neural Networks</a>
                                </li>
                                <li>
                                    <a class="nav-link " href="neural-net-tutorial.html">Intro to Neural Networks</a>
                                </li>
                                <li>
                                    <a class="nav-link" href="new-wards.html">New Wards, New Problems</a>
                                </li>
                            </ul>
                        </nav>
                    </div>
                </div>
                <div class="col-8" id="main">
                    <h1>Bayes By Backprop</h1>

                    <p>Bayes by Backprop is an algorithm for training Bayesian neural networks (what is a Bayesian neural network, you ask? Read more to find out), which was developed in the paper “Weight Uncertainty in Neural Networks” by Blundell et al. We will be using pytorch for this tutorial along with several standard python packages. I put this tutorial together with Joe Davison, Lucie Gillet, Baptiste Lemaire, and Robbert Struyven.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
                    <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
                    <span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
                    <span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
                    <span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Normal</span>
                    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
                    <span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
                    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
                    <span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s">'retina'</span>
                    </code></pre></div></div>

                    <h2 id="why-do-we-need-bayesian-neural-networks">Why do we need Bayesian Neural Networks</h2>

                    <p>There are three big problems with standard neural networks, which we’ll show with an example. Imagine you’re trying to fit a function to the following 6 datapoints.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">toy_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                      <span class="k">return</span> <span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>

                    <span class="c1"># toy dataset we can start with
                    </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">y</span> <span class="o">=</span> <span class="n">toy_function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Toy data datapoints'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
                    </code></pre></div></div>

                    <p><img src="output_4_0.png" class="img-fluid" alt="png" /></p>

                    <p>Let’s see what happens when we fit a standard multilayer perception with 2 layers and 8 hidden units each. Don’t worry too much about the code here - this is just to illustrate the behaviour of a typical neural network.</p>

                    <p><strong>Define standard MLP</strong></p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">standard_MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                          <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">l3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l3</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)))))</span>
                          <span class="k">return</span> <span class="n">x</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># initialization of our standard neural network
                    </span><span class="n">net1</span> <span class="o">=</span> <span class="n">standard_MLP</span><span class="p">()</span>

                    <span class="c1"># use of a Mean Square Error loss to evaluate the network because we are in a regression problem
                    </span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

                    <span class="c1"># use of stochastic gradient descent as our optimizer
                    </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

                    <span class="c1"># number of times we are looping over our data to train the network
                    </span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">30000</span>

                    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
                      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>   <span class="c1"># zero the gradient buffers
                    </span>    <span class="n">output</span> <span class="o">=</span> <span class="n">net1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># pass the data forward
                    </span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>   <span class="c1"># evaluate our performance
                    </span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                          <span class="k">print</span><span class="p">(</span><span class="s">"epoch {} loss: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="n">loss</span><span class="p">))</span>
                      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># calculates the gradients
                    </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>    <span class="c1"># updates weigths of the network
                    </span></code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch 0 loss: 6.118632793426514
epoch 5000 loss: 0.5939443111419678
epoch 10000 loss: 0.15989504754543304
epoch 15000 loss: 0.014005806297063828
epoch 20000 loss: 0.0007411101832985878
epoch 25000 loss: 4.190066101728007e-05
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">predictions</span> <span class="o">=</span> <span class="n">net1</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">predictions</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'nn predictions'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'true values'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Standard neural networks can overfit...'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
                    </code></pre></div></div>

                    <p><img src="output_8_1.png" class="img-fluid" alt="png" /></p>

                    <p>A perfect fit! But that is not necessarily a good thing. For instance, we are fitting an essentially straight line between <script type="math/tex">x = -1</script> and <script type="math/tex">x = 1</script>. The curved nature of the data suggests that this might not be correct. We are overfitting to the 6 data points. More worrying, look what happens when we extend our predictions beyond the range of our data.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">predictions</span> <span class="o">=</span> <span class="n">net1</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">predictions</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'nn predictions'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'true values'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'...and standard neural networks are overconfident in their generalizations'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

                    </code></pre></div></div>

                    <p><img src="output_10_1.png" class="img-fluid" alt="png" /></p>

                    <p>The neural network predicts -13 for $x = 100$, but there isn’t a data point in sight. We’d ideally want the uncertainty of our neural network to increase as it makes predictions further from the observed data.
                    To summarize, there are three drawbacks of traditional neural networks:</p>
                    <ul>
                    <li>The risk of overfitting your data</li>
                    <li>The lack of knowledge on how confident we are on our predictions</li>
                    <li>Overconfident generalization in areas without any data</li>
                    </ul>

                    <h2 id="what-are-bayesian-neural-networks">What are Bayesian Neural Networks?</h2>

                    <p>Bayesian neural networks solve these two problems by:</p>
                    <ol>
                    <li>Regularizing the network in a principled way</li>
                    <li>Predicting distributions instead of values which allows us to measure uncertainty and identify unfamiliar inputs</li>
                    </ol>

                    <p>To explain what a Bayesian neural network is, we need to rethink the usual machine learning problem framework. Typically, we are provided with a dataset $\mathcal{D} = \{x_i, y_i\}$ and we choose a model $f(X \vert w) = Y$ that is parameterized by weights $w$. To train our model, we adjust the weight to minimize a loss function $\mathcal{L}(f(X),Y)$.</p>

                    <p>As opposed to optimizing a loss function, bayesian neural networks take an explicitly probabilistic approach. Instead of assigning each weight $w_i$ as a single number, <strong>we model them with a probability distribution</strong>. The task is to find the probability distribution $P(w \vert \mathcal{D})$, which is called the posterior distribution. In other words, given our data, what are the weights of the neural network likely to be.</p>

                    <h3 id="what-do-bayesian-neural-networks-predict">What do Bayesian Neural Networks Predict?</h3>

                    <p>To make a “prediction” with a bayesian neural network, we are actually estimating an expected value $\mathbb{E}_{P(w \vert \mathcal{D})}\left[P(y \vert x, w)\right]$. Since the weights are now random variables, our “predictions” must be random variables too. If we want to make a single prediction as we did in the frequentist case, we calculate the expected value of $y$  given our final distributions over the weights, which in turn are dependent on our data.</p>

                    <p>We’ll use computers and not algebra to do this (as we’ll see in a moment, the math is too hard). In the same way that you estimate the expected value of a die by rolling it a bunch of time and averaging the values, we can estimate $\mathbb{E}_{P(w \vert \mathcal{D})}\left[P(y \vert x, w)\right]$ by sampling from the posterior distribution $P(w \vert \mathcal{D})$ and for each sample, calculate $f(X \vert w)$, where $f$ is our neural network as before. Finally, we average all of these estimates.</p>

                    <p><strong>Since our network outputs distributions rather than single values, it can communicate its certainty.</strong></p>

                    <h2 id="training-bayesian-neural-networks">Training Bayesian Neural Networks</h2>

                    <h3 id="attempt-1-analysis">Attempt 1: Analysis</h3>

                    <p>Now comes the tricky part: actually finding $P(w \vert \mathcal{D})$ . To write an explicit formula for the posterior, we would need to integrate over all possible neural network weights . Not gonna happen.</p>

                    <h3 id="attempt-2-sampling">Attempt 2: Sampling</h3>

                    <p>Another approach we could take is sampling from $P(w \vert \mathcal{D})$. This, however, would require sampling from a distribution with as many dimensions as there are neural network parameters. Neural networks often have thousands, if not millions, of parameters. Again, not gonna happen.</p>

                    <h3 id="attempt-3-variational-inference">Attempt 3: Variational Inference</h3>

                    <p>This leads us to variational inference and, in particular, Bayes by Backprop.</p>

                    <p><strong>Instead of trying to sample from a horrible multi-dimensional posterior, what if we approximate the posterior with a bunch of simple distributions?</strong></p>

                    <p>Specifically, let’s approximate each weight $w_i$ with a normal distribution with mean $\mu_i$ and standard deviation $\sigma_i$. We’ll denote all of these new parameters as $\theta = (\mu, \sigma)$, which defines a distribution over all the weights $q(w \vert \theta)$. Below is an image illustrating this idea. Notice that each weight $w_i$ is sampled from a normal distribution with mean $\mu_i$ and variance $\sigma_i$.</p>

                    <p><img src="https://raw.githubusercontent.com/zackchase/mxnet-the-straight-dope/master/img/bbb_nn_bayes.png" class="img-fluid" alt="Texte alternatif…" /></p>

                    <p><a href="https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html?fbclid=IwAR1Dwk4GaK64_EARPwUG5vmZR8lFYbekDN2k_eKvYviAxGcKAWWtBFYR1yA">source</a></p>

                    <p>We can’t just pick any $\mu$’s and $\sigma$’s - we need the distribution $q(w \vert \theta)$ to be “close” to our ultimate goal  $P(w \vert \mathcal{D})$. We can formalize this notion of “closeness” with the Kullback-Leibler (KL) divergence, which measures <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">something like</a> a distance between two distributions. <strong>Our goal, therefore, will be to minimize the KL divergence between $q(w \vert \theta)$ and $P(w \vert \mathcal{D})$.</strong> Written out in math, we are looking for $\theta^*$ such that,</p>

                    <script type="math/tex; mode=display">\theta^* = \underset{\theta}{\mathrm{argmin}} \text{ KL}\left[q(w \vert \theta) \vert \vert P(w \vert \mathcal{D})\right]</script>

                    <p>While variational inference forces us to make an approximation, a huge benefit of this approach is that we are back in the familiar optimization territory of machine learning.</p>

                    <h3 id="attempt-35-bayes-by-backprop">Attempt 3.5: Bayes By Backprop</h3>

                    <p>If we are optimizing parameters of a neural network (remember, these are parameters of the <em>distributions</em> of the weights), the natural question is if we can use backpropogation to do so. “Weight Uncertainty in Neural Networks” says almost.</p>

                    <p>Here’s the issue. Let’s write out the KL divergence between $q(w \vert \theta)$ and $P(w \vert \mathcal{D})$.</p>

                    <script type="math/tex; mode=display">% <![CDATA[
                    \begin{align}
                    \theta^* &= \underset{\theta}{\mathrm{argmin}} \text{ KL}\left[q(w \vert \theta) \vert \vert P(w \vert \mathcal{D})\right] & \\\\
                    &= \underset{\theta}{\mathrm{argmin}} \text{ }\mathbb{E}_{q(w \vert \theta)}\left[ \log\left[\frac{ q(w \vert \theta) }{P( w \vert \mathcal{D})}\right]\right] & \text{(definition of KL divegence)} \\\\
                    &= \underset{\theta}{\mathrm{argmin}} \text{ }\mathbb{E}_{q(w \vert \theta)}\left[ \log\left[\frac{ q(w \vert \theta)P(\mathcal{D}) }{P( \mathcal{D} \vert w)P(w)}\right]\right] & \text{(Bayes Theorem)} \\\\
                    &= \underset{\theta}{\mathrm{argmin}} \text{ }\mathbb{E}_{q(w \vert \theta)}\left[ \log\left[\frac{ q(w \vert \theta) }{P( \mathcal{D} \vert w)P(w)}\right]\right] & \text{(Drop }P(\mathcal{D})\text{ because it doesn't depend on } \theta)
                    \end{align} %]]></script>

                    <p>Let’s pause for a moment to understand the probabilities inside the expectation.</p>
                    <ul>
                    <li>$q(w \vert \theta)$ is the distribution of the weights given the parameters of the normal distribution that define them.</li>
                    <li>$P( \mathcal{D} \vert w)$ is the likelihood of observing our data given our weights (to calculate this we’ll need to compare our predictions to the actual data).</li>
                    <li>$P(w)$ is the prior we choose for the weights, which <strong>allows us to regularize our neural network in a principled way</strong>.</li>
                    </ul>

                    <p>Note: the negative of the value inside the expectation is called the evidence lower bound or the <strong>ELBO</strong>.</p>

                    <p>Nevertheless, we’ve hit a road block. If we want to use back propagation to minimize the KL divergence with respect to $\theta$, we’ll need to take a derivative of an expectation. The authors of “Weight Uncertainty in Neural Networks” suggest a neat trick to do this. We reparameterize our weights to be $w_i = \mu_i + \sigma_i
                    \times \epsilon_i$ where $\epsilon_i \sim \mathcal{N}(0,1)$. We can swap the derivative and the expectation because $w_i$ is a deterministic function of $\mu_i$ and $\sigma_i$ given $\epsilon_i$. In math this can be stated as:</p>

                    <script type="math/tex; mode=display">\frac{\partial}{\partial \theta}\mathbb{E}_{q(\epsilon)}\left[ \log\left[\frac{ q(w \vert \theta) }{P( \mathcal{D} \vert w)P(w)}\right]\right] =\mathbb{E}_{q(\epsilon)}\left[ \frac{\partial}{\partial \theta}\log\left[\frac{ q(w \vert \theta) }{P( \mathcal{D} \vert w)P(w)}\right]\right]</script>

                    <p>Notice that the expectation is with with respect to $\epsilon$ instead of $\theta$.</p>

                    <p><strong>This means we can estimate the gradient of the KL divergence with respect to $\theta$ by taking the average of $\frac{\partial}{\partial \theta}\log\left[\frac{ q(w \vert \theta) }{P( \mathcal{D} \vert w)P(w)}\right]$ for multiple samples of $\epsilon \sim \mathcal{N}(0,1)$.</strong></p>

                    <h2 id="the-bayes-by-backprop-algorithm">The Bayes by Backprop Algorithm</h2>

                    <p>One last thing before we go over the algorithm. We need to reparameterize $\sigma$ to the entire real line, not just positive values. We define $\sigma = \log(1+e^\rho)$, which means that $\theta = (\mu, \rho)$ (to emphasize, $\theta \neq (\mu, \sigma)$).</p>

                    <p><strong>At long last, here is bayes by backprop:</strong></p>

                    <p>As we explain the algorithm, we will show a single update for the function $y = wx$ in code with parameters $(\mu, \rho) = (2,3)$, prior $\mathcal{N}(0,1)$, noise tolerance 0.5, and a single data point $(x,y) = (1,1)$</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># initialize parameters
                    </span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="n">rho</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">]),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="mi">2</span>
                    <span class="n">y</span> <span class="o">=</span> <span class="mi">3</span>
                    <span class="n">noise_tol</span> <span class="o">=</span> <span class="mf">0.5</span>
                    <span class="n">prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
                    </code></pre></div></div>

                    <ol>
                    <li>Sample $\epsilon \sim \mathcal{N}(0,1)$ for every weight</li>
                    </ol>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epsilon</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                    <span class="k">print</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(1.9070)
                    </code></pre></div></div>

                    <ol>
                    <li>Let $w = \mu + \log(1+e^\rho)\times \epsilon$</li>
                    </ol>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">rho</span><span class="p">))</span> <span class="o">*</span> <span class="n">epsilon</span>
                    <span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4.5561], grad_fn=&lt;ThAddBackward&gt;)
                    </code></pre></div></div>

                    <ol>
                    <li>Let $\mathcal{L} = \sum_i \log q(w_i \vert \theta_i) - \sum_i \log P(w_i) - \sum_j \log P(y_j \vert w, x_j)$ where $q(w_i \vert \theta_i)$ is the log probability of $w_i$ given the normal distribution generated by $\theta$, $\log P(w_i)$ is the log probability of seeing $w_i$ as dictated by the prior distribution, and $\log P(y_j \vert w, x_i)$ is the log probability of observing data point $y_j$ given the weights and $x_i$. Note that this is the term inside the expecation above (i.e. the negative ELBO).</li>
                    </ol>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate variational posterior probability
                    </span><span class="n">log_variational_post</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">rho</span><span class="p">)))</span>
                    <span class="n">log_variational_post_prob</span> <span class="o">=</span> <span class="n">log_variational_post</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"Log Variational Posterior Prob:"</span><span class="p">,</span><span class="n">log_variational_post_prob</span><span class="p">)</span>

                    <span class="c1"># calculate prior probablility
                    </span><span class="n">log_prior_prob</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"Log Prior Prob:"</span><span class="p">,</span><span class="n">log_prior_prob</span><span class="p">)</span>

                    <span class="c1"># calculate likelihood
                    </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span>
                    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">noise_tol</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"Likelihood"</span><span class="p">,</span> <span class="n">log_likelihood</span><span class="p">)</span>

                    <span class="c1"># calculate negative ELBO
                    </span><span class="n">L</span> <span class="o">=</span> <span class="n">log_variational_post_prob</span> <span class="o">-</span> <span class="n">log_prior_prob</span> <span class="o">-</span> <span class="n">log_likelihood</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"Loss:"</span><span class="p">,</span><span class="n">L</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Log Variational Posterior Prob: tensor([-3.4920], grad_fn=&lt;SubBackward&gt;)
                    Log Prior Prob: tensor([-11.2981], grad_fn=&lt;SubBackward&gt;)
                    Likelihood tensor([-74.9450], grad_fn=&lt;SubBackward&gt;)
                    Loss: tensor([82.7511], grad_fn=&lt;ThSubBackward&gt;)
                    </code></pre></div></div>

                    <ol>
                    <li>Use your favourite automatic differentiation package to calculate $\nabla_\theta \mathcal{L}$</li>
                    </ol>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"mu grad:"</span><span class="p">,</span><span class="n">mu</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"rho grad:"</span><span class="p">,</span><span class="n">rho</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mu grad: tensor([53.4541])
                    rho grad: tensor([89.3733])
                    </code></pre></div></div>

                    <ol>
                    <li>Update $\theta’ = \theta -\alpha \nabla_\theta \mathcal{L}$</li>
                    </ol>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"old mu:"</span><span class="p">,</span><span class="n">mu</span><span class="p">)</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"old rho:"</span><span class="p">,</span><span class="n">rho</span><span class="p">)</span>
                    <span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">mu</span><span class="o">.</span><span class="n">grad</span>
                    <span class="n">rho</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">rho</span><span class="o">.</span><span class="n">grad</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"new mu:"</span><span class="p">,</span><span class="n">mu</span><span class="p">)</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"new rho:"</span><span class="p">,</span><span class="n">rho</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>old mu: tensor([0.5000], requires_grad=True)
                    old rho: tensor([2.], requires_grad=True)
                    new mu: tensor([-0.0345], grad_fn=&lt;ThSubBackward&gt;)
                    new rho: tensor([1.1063], grad_fn=&lt;ThSubBackward&gt;)
                    </code></pre></div></div>

                    <ol>
                    <li>Repeat 1-5 for a while</li>
                    </ol>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># see below!
                    </span></code></pre></div></div>

                    <p>Notice that we don’t actually take multiple samples of $\epsilon$ for a single gradient update and, instead, use only one realization. Over many interations, this shouldn’t be an issue.</p>

                    <h2 id="implementing-bayes-by-backprop">Implementing Bayes By Backprop</h2>

                    <p>We will use pytorch to implement the algorithm. First, we create a linear bayes by backprop layer, which will mimic the behaviour of a regular pytorch linear layer, but samples it’s outputs from probability distributions. Due to pytorch’s automatic differentiation functionality and our choice of parameterization, the gradients will be calculated automatically</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Linear_BBB</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                      <span class="s">"""
                          Layer of our BNN.
                      """</span>
                      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">,</span> <span class="n">prior_var</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
                          <span class="s">"""
                              Initialization of our layer : our prior is a normal distribution
                              centered in 0 and of variance 20.
                          """</span>
                          <span class="c1"># initialize layers
                    </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
                          <span class="c1"># set input and output dimensions
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">output_features</span> <span class="o">=</span> <span class="n">output_features</span>

                          <span class="c1"># initialize mu and rho parameters for the weights of the layer
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_features</span><span class="p">,</span> <span class="n">input_features</span><span class="p">))</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">w_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_features</span><span class="p">,</span> <span class="n">input_features</span><span class="p">))</span>

                          <span class="c1">#initialize mu and rho parameters for the layer's bias
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span> <span class="o">=</span>  <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_features</span><span class="p">))</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">b_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_features</span><span class="p">))</span>

                          <span class="c1">#initialize weight samples (these will be calculated whenever the layer makes a prediction)
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">None</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">None</span>

                          <span class="c1"># initialize prior distribution for all of the weights and biases
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">prior_var</span><span class="p">)</span>

                      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
                          <span class="s">"""
                            Optimization process
                          """</span>
                          <span class="c1"># sample weights
                    </span>        <span class="n">w_epsilon</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_rho</span><span class="p">))</span> <span class="o">*</span> <span class="n">w_epsilon</span>

                          <span class="c1"># sample bias
                    </span>        <span class="n">b_epsilon</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_rho</span><span class="p">))</span> <span class="o">*</span> <span class="n">b_epsilon</span>

                          <span class="c1"># record log prior by evaluating log pdf of prior at sampled weight and bias
                    </span>        <span class="n">w_log_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
                          <span class="n">b_log_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">log_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">w_log_prior</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">b_log_prior</span><span class="p">)</span>

                          <span class="c1"># record log variational posterior by evaluating log pdf of normal distribution defined by parameters with respect at the sampled values
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">w_post</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_rho</span><span class="p">)))</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">b_post</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_rho</span><span class="p">)))</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">log_post</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_post</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_post</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>

                          <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

                    </code></pre></div></div>

                    <p>Now that we’ve defined our bayes by backprop layer</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP_BBB</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">noise_tol</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">prior_var</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>

                          <span class="c1"># initialize the network like you would with a standard multilayer perceptron, but using the BBB layer
                    </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">Linear_BBB</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">prior_var</span><span class="o">=</span><span class="n">prior_var</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">Linear_BBB</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prior_var</span><span class="o">=</span><span class="n">prior_var</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">noise_tol</span> <span class="o">=</span> <span class="n">noise_tol</span> <span class="c1"># we will use the noise tolerance to calculate our likelihood
                    </span>
                      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                          <span class="c1"># again, this is equivalent to a standard multilayer perceptron
                    </span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                          <span class="k">return</span> <span class="n">x</span>

                      <span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                          <span class="c1"># calculate the log prior over all the layers
                    </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="o">.</span><span class="n">log_prior</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">log_prior</span>

                      <span class="k">def</span> <span class="nf">log_post</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                          <span class="c1"># calculate the log posterior over all the layers
                    </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="o">.</span><span class="n">log_post</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">log_post</span>

                      <span class="k">def</span> <span class="nf">sample_elbo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
                          <span class="c1"># we calculate the negative elbo, which will be our loss function
                    </span>        <span class="c1">#initialize tensors
                    </span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                          <span class="n">log_priors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
                          <span class="n">log_posts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
                          <span class="n">log_likes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
                          <span class="c1"># make predictions and calculate prior, posterior, and likelihood for a given number of samples
                    </span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                              <span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># make predictions
                    </span>            <span class="n">log_priors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prior</span><span class="p">()</span> <span class="c1"># get log prior
                    </span>            <span class="n">log_posts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_post</span><span class="p">()</span> <span class="c1"># get log variational posterior
                    </span>            <span class="n">log_likes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_tol</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="c1"># calculate the log likelihood
                    </span>        <span class="c1"># calculate monte carlo estimate of prior posterior and likelihood
                    </span>        <span class="n">log_prior</span> <span class="o">=</span> <span class="n">log_priors</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                          <span class="n">log_post</span> <span class="o">=</span> <span class="n">log_posts</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                          <span class="n">log_like</span> <span class="o">=</span> <span class="n">log_likes</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                          <span class="c1"># calculate the negative elbo (which is our loss function)
                    </span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">log_post</span> <span class="o">-</span> <span class="n">log_prior</span> <span class="o">-</span> <span class="n">log_like</span>
                          <span class="k">return</span> <span class="n">loss</span>

                    </code></pre></div></div>

                    <h2 id="train-net">Train Net</h2>
                    <p>We can now train our bayesian neural network on the toy data set and observe the differences between the Bayesian approach and the usual frequentist approach.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">MLP_BBB</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">prior_var</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epochs</span> <span class="o">=</span> <span class="mi">2000</span>
                    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times
                    </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                      <span class="c1"># forward + backward + optimize
                    </span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">sample_elbo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                      <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                          <span class="k">print</span><span class="p">(</span><span class="s">'epoch: {}/{}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="p">))</span>
                          <span class="k">print</span><span class="p">(</span><span class="s">'Loss:'</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">'Finished Training'</span><span class="p">)</span>

                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch: 1/2000
                    Loss: 3808.865478515625
                    ...
                    epoch: 1991/2000
                    Loss: 278.11016845703125
                    Finished Training
                    </code></pre></div></div>

                    <p>Done training! Let’s see our results</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># samples is the number of "predictions" we make for 1 x-value.
                    </span><span class="n">samples</span> <span class="o">=</span> <span class="mi">100</span>
                    <span class="n">x_tmp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">y_samp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">samples</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                      <span class="n">y_tmp</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x_tmp</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                      <span class="n">y_samp</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_tmp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_tmp</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_samp</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'Mean Posterior Predictive'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_tmp</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y_samp</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y_samp</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'95</span><span class="si">% </span><span class="s">Confidence'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">toy_function</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Posterior Predictive'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
                    </code></pre></div></div>

                    <p><img src="output_35_0.png" class="img-fluid" alt="png" /></p>

                    <p>We can see that now there is a bit of uncertainty around the curve in the data. This can be compared to the frequentist neural network that confidently overfit a line exactly through the data points. Now let’s zoom out.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="mi">100</span>
                    <span class="n">x_tmp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">y_samp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">samples</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                      <span class="n">y_tmp</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x_tmp</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                      <span class="n">y_samp</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_tmp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_tmp</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_samp</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'Mean Posterior Predictive'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_tmp</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y_samp</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y_samp</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'95</span><span class="si">% </span><span class="s">Confidence'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">toy_function</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Posterior Predictive'</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
                    </code></pre></div></div>

                    <p><img src="output_37_0.png" class="img-fluid" alt="png" /></p>

                    <p>This is very different from the frequentist neural network. With the bayesian neural network, in the regions where there is no data, the model predicts a wide probability distribution instead of a single point.</p>

                    <h2 id="bbb-for-handwritten-digit-classification">BbB for Handwritten Digit Classification</h2>

                    <p>Now that we have a good grasp of BbB and its workings from our toy dataset, let’s try it on a real-world problem: hand-written digit classification. The MNIST dataset consists of 70,000 28x28 black and white digits from 0-9. Modern neural networks are able to effectively classify these digits, but suffer from overconfidence on out-of-domain (OOD) data. For example, random noise typically causes very high confidence in one particular class. Idealized BNNs are immune to this problem due to the theoretically balanced posterior predictive distribution in low-density data regions. Unfortunately, finding such <em>idealized</em> BNNs is not possible, but we can use BbB to approximate this distribution.</p>

                    <p>Let’s start by preparing our MNIST dataset for training:</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
                      <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s">'data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                     <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                                         <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                         <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span>
                                         <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span>
                                     <span class="p">])),</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
                      <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s">'data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                                         <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                         <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span>
                                         <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span>
                                     <span class="p">])),</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
                    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
                    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
                    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
                    Processing...
                    Done!
                    </code></pre></div></div>

                    <p>Once we have our BNN model, we’ll want something to compare our results against. To this end, we’ll train a standard neural network with an MLE parameter configuration. This will allow us to benchmark both our accuracy and our predictive uncertainty.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Classifier_MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
                          <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">h1</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">h2</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>

                      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                          <span class="k">return</span> <span class="n">x</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_size</span>    <span class="o">=</span> <span class="mi">784</span>   <span class="c1"># The image size = 28 x 28 = 784
                    </span><span class="n">hidden_size</span>   <span class="o">=</span> <span class="mi">400</span>   <span class="c1"># The number of nodes at the hidden layer
                    </span><span class="n">num_classes</span>   <span class="o">=</span> <span class="mi">10</span>    <span class="c1"># The number of output classes. In this case, from 0 to 9
                    </span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>  <span class="c1"># The speed of convergence
                    </span>
                    <span class="n">MLP</span> <span class="o">=</span> <span class="n">Classifier_MLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
                    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">MLP</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
                    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times
                    </span>    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
                          <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                          <span class="n">pred</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
                          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
                          <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                          <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                      <span class="n">learning_rate</span> <span class="o">/=</span> <span class="mf">1.1</span>
                      <span class="n">test_losses</span><span class="p">,</span> <span class="n">test_accs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
                      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
                          <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                          <span class="n">pred</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
                          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
                          <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                          <span class="n">test_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                          <span class="n">test_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                      <span class="k">print</span><span class="p">(</span><span class="s">'Loss: {}, Accuracy: {}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_losses</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accs</span><span class="p">)))</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">'Finished Training'</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
                    if sys.path[0] == '':


                    Loss: 0.12482864770425271, Accuracy: 0.964003164556962
                    Loss: 0.090960940322544, Accuracy: 0.9715189873417721
                    Loss: 0.08010370771200219, Accuracy: 0.974881329113924
                    Loss: 0.06610545960455379, Accuracy: 0.9791337025316456
                    Loss: 0.07792819705880188, Accuracy: 0.9783425632911392
                    Loss: 0.07006107058517541, Accuracy: 0.9785403481012658
                    Loss: 0.10337369290145138, Accuracy: 0.9742879746835443
                    Loss: 0.07452931634409801, Accuracy: 0.9805181962025317
                    Loss: 0.07194906106819929, Accuracy: 0.982693829113924
                    Loss: 0.08781571256857412, Accuracy: 0.9800237341772152
                    Finished Training
                    </code></pre></div></div>

                    <p>Now we can train BbB, but first we need to make a couple of  update our loss function for classification as opposed to regression. With regression, we imposed a Gaussian likelihood function over our predictions. For classification, we can simply rely on the softmax activation function on the output layer to turn our predictions into proper probabilities. Our likelihood function then becomes the crossentropy of the predictions and the true labels. For two classes, this comes to:</p>

                    <script type="math/tex; mode=display">p(\mathbf{y}|\mathbf{x},w) -\sum_i y_i \log (f(x_i|w)) + (1-y_i) \log(1-f(x_i|w))</script>

                    <p>Where <script type="math/tex">f(x_i \vert w)</script> is the softmax output for <script type="math/tex">x_i</script>.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Linear_BBB</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                      <span class="s">"""
                          Layer of our BNN.
                      """</span>
                      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">,</span> <span class="n">prior_var</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
                          <span class="s">"""
                              Initialization of our layer : our prior is a normal distribution
                              centered in 0 and of variance 20.
                          """</span>
                          <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
                          <span class="c1">#set dim
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">output_features</span> <span class="o">=</span> <span class="n">output_features</span>

                          <span class="c1"># initialize weight params
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_features</span><span class="p">,</span> <span class="n">input_features</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">))</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">w_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_features</span><span class="p">,</span> <span class="n">input_features</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">6</span><span class="p">))</span>

                          <span class="c1">#initialize bias params
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span> <span class="o">=</span>  <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_features</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">))</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">b_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_features</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">6</span><span class="p">))</span>

                          <span class="c1">#initialize weight samples
                    </span>        <span class="n">w_epsilon</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_rho</span><span class="p">))</span> <span class="o">*</span> <span class="n">w_epsilon</span>
                          <span class="n">b_epsilon</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_rho</span><span class="p">))</span> <span class="o">*</span> <span class="n">b_epsilon</span>

                          <span class="c1"># initialize prior distribution
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">prior_var</span><span class="p">)</span>

                      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
                          <span class="s">"""
                            Optimization process
                          """</span>
                          <span class="c1">#sample weights
                    </span>        <span class="n">w_epsilon</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_rho</span><span class="p">))</span> <span class="o">*</span> <span class="n">w_epsilon</span>

                          <span class="c1">#sample bias
                    </span>        <span class="n">b_epsilon</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_rho</span><span class="p">))</span> <span class="o">*</span> <span class="n">b_epsilon</span>

                          <span class="c1">#record prior
                    </span>        <span class="n">w_log_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
                          <span class="n">b_log_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">log_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">w_log_prior</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">b_log_prior</span><span class="p">)</span>

                          <span class="c1">#record variational_posterior
                    </span>        <span class="bp">self</span><span class="o">.</span><span class="n">w_post</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_mu</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_rho</span><span class="p">)))</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">b_post</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_mu</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_rho</span><span class="p">)))</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">log_post</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_post</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_post</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>

                          <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Classifier_BBB</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
                          <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">h1</span>  <span class="o">=</span> <span class="n">Linear_BBB</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">h2</span>  <span class="o">=</span> <span class="n">Linear_BBB</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">Linear_BBB</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>

                      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                          <span class="c1">#x = x.view(-1, 28*28)
                    </span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                          <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                          <span class="k">return</span> <span class="n">x</span>

                      <span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                          <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">h1</span><span class="o">.</span><span class="n">log_prior</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">h2</span><span class="o">.</span><span class="n">log_prior</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">log_prior</span>

                      <span class="k">def</span> <span class="nf">log_post</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                          <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">h1</span><span class="o">.</span><span class="n">log_post</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">h2</span><span class="o">.</span><span class="n">log_post</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">log_post</span>

                      <span class="k">def</span> <span class="nf">sample_elbo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
                          <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
                          <span class="n">log_priors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
                          <span class="n">log_posts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
                          <span class="c1">#log_likes = torch.zeros(samples)
                    </span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                            <span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
                            <span class="n">log_priors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prior</span><span class="p">()</span>
                            <span class="n">log_posts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_post</span><span class="p">()</span>
                            <span class="c1">#log_likes[i] = torch.log(outputs[i, torch.arange(outputs.shape[1]), target]).sum(dim=-1)
                    </span>        <span class="n">log_prior</span> <span class="o">=</span> <span class="n">log_priors</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                          <span class="n">log_post</span> <span class="o">=</span> <span class="n">log_posts</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                          <span class="c1">#log_likes = F.nll_loss(outputs.mean(0), target, size_average=False)
                    </span>        <span class="n">log_likes</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>
                          <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_post</span> <span class="o">-</span> <span class="n">log_prior</span><span class="p">)</span><span class="o">/</span><span class="n">num_batches</span> <span class="o">+</span> <span class="n">log_likes</span>
                          <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_size</span>    <span class="o">=</span> <span class="mi">784</span>   <span class="c1"># The image size = 28 x 28 = 784
                    </span><span class="n">hidden_size</span>   <span class="o">=</span> <span class="mi">512</span>   <span class="c1"># The number of nodes at the hidden layer
                    </span><span class="n">num_classes</span>   <span class="o">=</span> <span class="mi">10</span>    <span class="c1"># The number of output classes. In this case, from 0 to 9
                    </span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>
                    <span class="n">num_batches</span>   <span class="o">=</span> <span class="mi">60000</span> <span class="o">/</span> <span class="n">batch_size</span>

                    <span class="n">classifier</span> <span class="o">=</span> <span class="n">Classifier_BBB</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
                    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
                    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times
                    </span>    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
                          <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                          <span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">sample_elbo</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                          <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                          <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                      <span class="n">learning_rate</span> <span class="o">/=</span> <span class="mf">1.1</span>
                      <span class="n">test_losses</span><span class="p">,</span> <span class="n">test_accs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
                      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
                          <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">sample_elbo</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
                          <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">test_pred</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                          <span class="n">test_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                          <span class="n">test_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                      <span class="k">print</span><span class="p">(</span><span class="s">'Loss: {}, Accuracy: {}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_losses</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accs</span><span class="p">)))</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">'Finished Training'</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Loss: 9806.669921875, Accuracy: 0.9107001582278481
                    ...
                    Loss: 4913.42324713212, Accuracy: 0.9725079113924051
                    Finished Training
                    </code></pre></div></div>

                    <p>Now that we have our trained models, we need to evaluate and compare them. Unfortunately, we can’t simply plot the posterior predictive to empirically judge the model performance due to the multidimensionality of the data. The approach we will take instead is measuring the predictive entropy of out-of-domain inputs.</p>

                    <p>Since our model was trained on images of handwritten digits, how should it respond if it sees something completely dissimilar to a digit? Ideally, it would express a complete lack of confidence by predicting each class with equal probability. This is the idea we will use to evaluate our model. We trained on MNIST, so we will try plugging in non-MNIST images and analize our model’s distribution of predictions. If it predicts one class with high confidence relative to a standard neural network, this is an indicator of poor performance. If, on the other hand, it shows a relatively even distribution over outputs, this signifies that the model has high uncertainty with OOD inputs, which is our goal. In addition to manually inspecting the outputs, as we shall do below, we can measure this quantitatively by the computing entropy of the softmax outputs across our OOD images:</p>

                    <script type="math/tex; mode=display">\dfrac{1}{N}\sum_i^N\sum_k^Klog(p(x_i=k))p(x_i=k)</script>

                    <p>Where $N$ is the number of test images and $K$ is the number of classes our model predicts.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_entropy</span><span class="p">(</span><span class="n">preds</span><span class="p">):</span>
                      <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">preds</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="o">*</span> <span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1000</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mlp_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">MLP</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate predictions for 1000 random noise images with 100 MC samples each
                    </span><span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
                      <span class="n">preds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mlp_entropy</span> <span class="o">=</span> <span class="n">compute_entropy</span><span class="p">(</span><span class="n">mlp_preds</span><span class="p">)</span>
                    <span class="n">bnn_entropy</span> <span class="o">=</span> <span class="n">compute_entropy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
                    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'MLP Entropy: {mlp_entropy:0.4f}</span><span class="se">\n</span><span class="s">BNN Entropy: {bnn_entropy:0.4f}'</span><span class="p">)</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MLP Entropy: 0.1446
                    BNN Entropy: 0.9909
                    </code></pre></div></div>

                    <p>Good news! After computing the predictive entropy of both a standard MLP and a BNN, the BNN finds significantly higher entropy on our OOD test data. This indicates that the BNN is <em>more uncertain</em> about its predictions on garbage inputs than the MLP.</p>

                    <p>To get get a better feel for these results, we can plot the predictions for both models for a number of OOD images and compare their predictive uncertainties.</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
                    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
                    <span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="n">mpimg</span>
                    <span class="kn">from</span> <span class="nn">skimage.transform</span> <span class="kn">import</span> <span class="n">resize</span>

                    <span class="k">def</span> <span class="nf">rgb2gray</span><span class="p">(</span><span class="n">rgb</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
                      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rgb</span><span class="p">[</span><span class="o">...</span><span class="p">,:</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">])</span>
                    <span class="k">if</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">4</span><span class="p">:</span>
                      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rgb</span><span class="p">[</span><span class="o">...</span><span class="p">,:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">])</span>

                    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">))</span>
                    <span class="n">axs</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

                    <span class="n">names</span><span class="o">=</span><span class="p">[</span> <span class="s">"2"</span><span class="p">,</span>  <span class="s">"3"</span> <span class="p">,</span> <span class="s">"7"</span><span class="p">,</span> <span class="s">"9"</span><span class="p">,</span> <span class="s">"Bayes"</span><span class="p">,</span> <span class="s">"Fisher"</span><span class="p">,</span> <span class="s">"Wilfred Cat"</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">filename</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">names</span><span class="p">):</span>
                    <span class="n">img</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"gdrive/My Drive/"</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">".png"</span><span class="p">)</span>
                    <span class="n">gray</span> <span class="o">=</span> <span class="n">rgb2gray</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]:</span>
                      <span class="n">gray</span><span class="o">=-</span><span class="n">gray</span>
                    <span class="n">st</span><span class="o">=</span><span class="p">(</span><span class="n">gray</span><span class="o">-</span><span class="n">gray</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">/</span><span class="n">gray</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
                    <span class="n">st_rescaled</span><span class="o">=</span><span class="n">resize</span><span class="p">(</span><span class="n">st</span><span class="p">,(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span><span class="n">mode</span><span class="o">=</span><span class="s">'reflect'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Original: '</span> <span class="o">+</span> <span class="n">filename</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Gray: '</span> <span class="o">+</span> <span class="n">filename</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">st</span><span class="p">);</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Rescaled: '</span><span class="o">+</span> <span class="n">filename</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">st_rescaled</span><span class="p">)</span>
                    <span class="n">pred</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
                      <span class="n">pred</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">=</span><span class="n">classifier</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">st_rescaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)))</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'BNN Predictions'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Class'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Probabilities'</span><span class="p">)</span>

                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'MLP Predictions'</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Class'</span><span class="p">)</span>
                    <span class="n">mlp_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">MLP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">st_rescaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">))))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">mlp_pred</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
                    <span class="n">axs</span><span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Probabilities'</span><span class="p">)</span>
                    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
                    </code></pre></div></div>

                    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
                    del sys.path[0]
                    /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
                    if sys.path[0] == '':
                    </code></pre></div></div>

                    <p><img src="output_60_1.png" class="img-fluid" alt="png" /></p>

                    <p>We can also do the same thing for our random noise inputs that we used to calculate entropy:</p>

                    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
                    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
                    <span class="c1"># output=np.zeros((100,8))
                    </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">),</span> <span class="mi">8</span><span class="p">)):</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Random noise image'</span><span class="p">)</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'BNN Predictions'</span><span class="p">)</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Class'</span><span class="p">)</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">preds</span><span class="p">[:,</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Predicted Probability'</span><span class="p">)</span>

                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'MLP Predictions'</span><span class="p">)</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Class'</span><span class="p">)</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">mlp_preds</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
                      <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Predicted Probability'</span><span class="p">)</span>
                    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
                    </code></pre></div></div>

                    <p><img src="output_62_0.png" class="img-fluid" alt="png" /></p>

                    <p>As we can see, the BNN outputs show much more uncertainty for OOD inputs than the deterministic MLP. In a perfect scenario, an idealized BNN would output nearly uniform probabilities for each of the above images. Our approximation $q$ of the true posterior is limited, however, by the expressiveness of the variational family, the choice of prior, and the difficulty of optimization in high-dimensional spaces. However, BbB sees considerable improvement over the MLP in predictive uncertainty measurements, which indicates that it serves as a much better approximation of the true posterior than a simple MLE estimator (i.e., a standard neural network) could achieve.</p>
                </div>
            </div>
        </div>
    </body>
</html>
